{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import scipy as scp\n",
    "import scipy.stats as scps\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.optimize import minimize\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load my own functions\n",
    "import dnnregressor_train_eval_keras as dnnk\n",
    "import make_data_wfpt as mdw\n",
    "from kde_training_utilities import kde_load_data\n",
    "import ddm_data_simulation as ddm_sim\n",
    "import boundary_functions as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5069085281283002902\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17258515983273140430\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 12048773940\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12938403459843521874\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7595569028926069205\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Handle some cuda business\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/afengler/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f92b4632320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "model_path = '/media/data_cifs/afengler/data/kde/linear_collapse/keras_models/dnnregressor_ddm_linear_collapse_06_22_19_23_27_28/model_0' \n",
    "ckpt_path = '/media/data_cifs/afengler/data/kde/linear_collapse/keras_models/dnnregressor_ddm_linear_collapse_06_22_19_23_27_28/ckpt_0_130'\n",
    "\n",
    "model = keras.models.load_model(model_path)\n",
    "model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations -----\n",
    "n_runs = 100\n",
    "n_samples = 2500\n",
    "feature_file_path = '/media/data_cifs/afengler/data/kde/linear_collapse/train_test_data/test_features.pickle'\n",
    "mle_out_path = '/media/data_cifs/afengler/data/kde/linear_collapse/mle_runs'\n",
    "\n",
    "# NOTE PARAMETERS: WEIBULL: [v, a, w, node, shape, scale]\n",
    "param_bounds = [(-1, 1), (0.3, 2), (0.3, 0.7), (0.01, 0.01), (0, np.pi / 2.2)]\n",
    "\n",
    "\n",
    "my_optim_columns = ['v_sim', 'a_sim', 'w_sim', 'node_sim', 'theta_sim',\n",
    "                    'v_mle', 'a_mle', 'w_mle', 'node_mle', 'theta_mle', 'n_samples']\n",
    "\n",
    "# Get parameter names in correct ordering:\n",
    "dat = pickle.load(open(feature_file_path, \n",
    "                       'rb'))\n",
    "\n",
    "parameter_names = list(dat.keys())[:-2] # :-1 to get rid of 'rt' and 'choice' here\n",
    "\n",
    "# Make columns for optimizer result table\n",
    "p_sim = []\n",
    "p_mle = []\n",
    "\n",
    "for parameter_name in parameter_names:\n",
    "    p_sim.append(parameter_name + '_sim')\n",
    "    p_mle.append(parameter_name + '_mle')\n",
    "    \n",
    "my_optim_columns = p_sim + p_mle + ['n_samples']\n",
    "\n",
    "# Initialize the data frame in which to store optimizer results\n",
    "optim_results = pd.DataFrame(np.zeros((n_runs, len(my_optim_columns))), columns = my_optim_columns)\n",
    "optim_results.iloc[:, 2 * len(parameter_names)] = n_samples\n",
    "\n",
    "# define boundary\n",
    "boundary = bf.linear_collapse\n",
    "boundary_multiplicative = False\n",
    "\n",
    "# Define the likelihood function\n",
    "def log_p(params = [0, 1, 0.9], model = [], data = [], parameter_names = []):\n",
    "    # Make feature array\n",
    "    feature_array = np.zeros((data[0].shape[0], len(parameter_names) + 2))\n",
    "    \n",
    "    # Store parameters\n",
    "    cnt = 0\n",
    "    for i in range(0, len(parameter_names), 1):\n",
    "        feature_array[:, i] = params[i]\n",
    "        cnt += 1\n",
    "    \n",
    "    # Store rts and choices\n",
    "    feature_array[:, cnt] = data[0].ravel() # rts\n",
    "    feature_array[:, cnt + 1] = data[1].ravel() # choices\n",
    "    \n",
    "    # Get model predictions\n",
    "    prediction = model.predict(feature_array)\n",
    "    \n",
    "    # Some post-processing of predictions\n",
    "    prediction[prediction < 1e-29] = 1e-29\n",
    "    \n",
    "    return(- np.sum(np.log(prediction)))  \n",
    "\n",
    "def make_params(param_bounds = []):\n",
    "    params = np.zeros(len(param_bounds))\n",
    "    \n",
    "    for i in range(len(params)):\n",
    "        params[i] = np.random.uniform(low = param_bounds[i][0], high = param_bounds[i][1])\n",
    "        \n",
    "    return params\n",
    "# ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop ----------- TD: Parallelize\n",
    "for i in range(0, n_runs, 1): \n",
    "    \n",
    "    # Get start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "#     # Sample parameters\n",
    "#     v_sim = np.random.uniform(high = v_range[1], low = v_range[0])\n",
    "#     a_sim = np.random.uniform(high = a_range[1], low = a_range[0])\n",
    "#     w_sim = np.random.uniform(high = w_range[1], low = w_range[0])\n",
    "\n",
    "#     #c1_sim = np.random.uniform(high = c1_range[1], low = c1_range[0])\n",
    "#     #c2_sim = np.random.uniform(high = c2_range[1], low = c2_range[0])\n",
    "#     node_sim = np.random.uniform(high = node_range[1], low = node_range[0])\n",
    "#     shape_sim = np.random.uniform(high = shape_range[1], low = shape_range[0])\n",
    "#     scale_sim = np.random.uniform(high = scale_range[1], low = scale_range[0])\n",
    "\n",
    "    tmp_params = make_params(param_bounds = param_bounds)\n",
    "    \n",
    "    # Store in output file\n",
    "    optim_results.iloc[i, :len(parameter_names)] = tmp_params\n",
    "    \n",
    "    # Print some info on run\n",
    "    print('Parameters for run ' + str(i) + ': ')\n",
    "    print(tmp_params)\n",
    "    \n",
    "    # Define boundary params\n",
    "    boundary_params = {'node': tmp_params[3],\n",
    "                       'theta': tmp_params[4]}\n",
    "    \n",
    "    # Run model simulations\n",
    "    ddm_dat_tmp = ddm_sim.ddm_flexbound_simulate(v = tmp_params[0],\n",
    "                                                 a = tmp_params[1],\n",
    "                                                 w = tmp_params[2],\n",
    "                                                 s = 1,\n",
    "                                                 delta_t = 0.001,\n",
    "                                                 max_t = 20,\n",
    "                                                 n_samples = n_samples,\n",
    "                                                 boundary_fun = boundary, # function of t (and potentially other parameters) that takes in (t, *args)\n",
    "                                                 boundary_multiplicative = boundary_multiplicative, # CAREFUL: CHECK IF BOUND\n",
    "                                                 boundary_params = boundary_params)\n",
    "        \n",
    "    # Print some info on run\n",
    "    print('Mean rt for current run: ')\n",
    "    print(np.mean(ddm_dat_tmp[0]))\n",
    "    \n",
    "    # Run optimizer\n",
    "    out = differential_evolution(log_p, \n",
    "                                 bounds = param_bounds, \n",
    "                                 args = (model, ddm_dat_tmp, parameter_names), \n",
    "                                 popsize = 30,\n",
    "                                 disp = True)\n",
    "    \n",
    "    # Print some info\n",
    "    print('Solution vector of current run: ')\n",
    "    print(out.x)\n",
    "    \n",
    "    print('The run took: ')\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    \n",
    "    # Store result in output file\n",
    "    optim_results.iloc[i, len(parameter_names):(2*len(parameter_names))] = out.x\n",
    "# -----------------------\n",
    "\n",
    "# Save optimization results to file\n",
    "optim_results.to_csv(mle_out_path + '/mle_results_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in results\n",
    "optim_results = pd.read_csv(os.getcwd() + '/experiments/ddm_flexbound_kde_mle_fix_v_0_c1_0_w_unbiased_arange_2_3/optim_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['v_sim'], optim_results['v_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for v\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['v_mle'], 1), np.expand_dims(optim_results['v_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['a_sim'], optim_results['a_mle'], c = optim_results['c2_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for a\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['a_mle'], 1), np.expand_dims(optim_results['a_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['w_sim'], optim_results['w_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['w_mle'], 1), np.expand_dims(optim_results['w_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c1_sim'], optim_results['c1_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for c1\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c1_mle'], 1), np.expand_dims(optim_results['c1_sim'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(optim_results['c2_sim'], optim_results['c2_mle'], c = optim_results['a_mle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression for w\n",
    "reg = LinearRegression().fit(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))\n",
    "reg.score(np.expand_dims(optim_results['c2_mle'], 1), np.expand_dims(optim_results['c2_sim'], 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
